import { execFileSync } from "node:child_process";
import fs from "node:fs/promises";
import os from "node:os";
import path from "node:path";

const GDEVELOP_REPOSITORY_URL = "https://github.com/4ian/GDevelop";
const AUTO_GENERATED_PATTERNS = [
  /auto-generated reference page/i,
  /auto generated reference page/i,
  /autogenerated/i,
  /auto-generated/i,
  /do not edit/i,
  /this file is generated/i,
  /generated file/i,
];

const BINARY_FILE_EXTENSIONS = new Set([
  ".png",
  ".jpg",
  ".jpeg",
  ".gif",
  ".webp",
  ".svg",
  ".ico",
  ".mp3",
  ".ogg",
  ".wav",
  ".mp4",
  ".webm",
  ".zip",
  ".7z",
  ".rar",
  ".pdf",
  ".ttf",
  ".otf",
  ".woff",
  ".woff2",
  ".eot",
  ".psd",
  ".blend",
  ".unitypackage",
  ".jar",
  ".exe",
  ".dll",
  ".so",
  ".dylib",
  ".wasm",
]);

const STOP_WORDS = new Set([
  "from",
  "with",
  "this",
  "that",
  "will",
  "have",
  "into",
  "your",
  "their",
  "about",
  "docs",
  "doc",
  "gdevelop",
  "index",
  "file",
  "files",
  "update",
  "updates",
  "feature",
  "features",
  "code",
  "main",
  "core",
  "tests",
  "test",
]);

export function runCommand(command, args, options = {}) {
  const { cwd, allowFailure = false } = options;
  try {
    return execFileSync(command, args, {
      cwd,
      encoding: "utf8",
      stdio: ["ignore", "pipe", "pipe"],
    });
  } catch (error) {
    if (allowFailure) {
      return null;
    }
    const stderr = error.stderr ? String(error.stderr).trim() : "";
    const joinedArgs = args.join(" ");
    const details = stderr ? `\n${stderr}` : "";
    throw new Error(`Command failed: ${command} ${joinedArgs}${details}`);
  }
}

export function runGit(repositoryPath, args, options = {}) {
  return runCommand("git", ["-C", repositoryPath, ...args], options);
}

export function toPosixPath(filePath) {
  return filePath.split(path.sep).join("/");
}

export function isAutoGeneratedContent(content) {
  return AUTO_GENERATED_PATTERNS.some((pattern) => pattern.test(content));
}

export function isValidCommitHash(value) {
  return typeof value === "string" && /^[a-f0-9]{7,40}$/i.test(value);
}

function normalizeData(data) {
  const normalized = {
    last_automated_updates_commit: "",
    last_improved_things: [],
  };

  if (
    data &&
    typeof data.last_automated_updates_commit === "string" &&
    isValidCommitHash(data.last_automated_updates_commit)
  ) {
    normalized.last_automated_updates_commit =
      data.last_automated_updates_commit;
  }

  if (data && Array.isArray(data.last_improved_things)) {
    normalized.last_improved_things = data.last_improved_things
      .filter((value) => typeof value === "string")
      .map((value) => value.trim())
      .filter(Boolean);
  }

  return normalized;
}

export async function ensureAutomationDataFile(repositoryRoot) {
  const dataFilePath = path.join(repositoryRoot, "automated_updates_data.json");
  const defaultData = normalizeData({});

  try {
    const existingRawData = await fs.readFile(dataFilePath, "utf8");
    const parsedData = JSON.parse(existingRawData);
    const normalizedData = normalizeData(parsedData);
    const shouldRewrite =
      JSON.stringify(parsedData) !== JSON.stringify(normalizedData);

    if (shouldRewrite) {
      await fs.writeFile(
        dataFilePath,
        `${JSON.stringify(normalizedData, null, 2)}\n`,
        "utf8",
      );
    }

    return { dataFilePath, data: normalizedData };
  } catch (error) {
    if (error && error.code !== "ENOENT") {
      throw error;
    }

    await fs.writeFile(
      dataFilePath,
      `${JSON.stringify(defaultData, null, 2)}\n`,
      "utf8",
    );
    return { dataFilePath, data: defaultData };
  }
}

export async function saveAutomationData(dataFilePath, data) {
  const normalizedData = normalizeData(data);
  await fs.writeFile(
    dataFilePath,
    `${JSON.stringify(normalizedData, null, 2)}\n`,
    "utf8",
  );
}

export async function cloneGDevelopRepository() {
  const tempDirectory = await fs.mkdtemp(
    path.join(os.tmpdir(), "gdevelop-source-"),
  );
  const repositoryPath = path.join(tempDirectory, "GDevelop");

  runCommand("git", [
    "clone",
    "--quiet",
    "--filter=blob:none",
    GDEVELOP_REPOSITORY_URL,
    repositoryPath,
  ]);

  const latestCommit = runGit(repositoryPath, ["rev-parse", "HEAD"])
    .trim()
    .toLowerCase();

  return {
    repositoryPath,
    latestCommit,
    cleanup: async () => {
      await fs.rm(tempDirectory, { recursive: true, force: true });
    },
  };
}

export function commitExists(repositoryPath, commitHash) {
  if (!isValidCommitHash(commitHash)) {
    return false;
  }
  const result = runGit(
    repositoryPath,
    ["cat-file", "-e", `${commitHash}^{commit}`],
    { allowFailure: true },
  );
  return result !== null;
}

export function getCommitsToInspect(
  repositoryPath,
  lastAutomatedCommit,
  defaultCommitCount,
) {
  const safeDefaultCount = Number.isInteger(defaultCommitCount)
    ? Math.max(defaultCommitCount, 1)
    : 5;
  const normalizedLastCommit = String(lastAutomatedCommit || "").trim();

  if (
    normalizedLastCommit &&
    commitExists(repositoryPath, normalizedLastCommit.toLowerCase())
  ) {
    const rangeText =
      runGit(repositoryPath, [
        "rev-list",
        "--reverse",
        `${normalizedLastCommit}..HEAD`,
      ]) || "";
    return rangeText
      .split("\n")
      .map((line) => line.trim().toLowerCase())
      .filter(Boolean);
  }

  const fallbackText =
    runGit(repositoryPath, [
      "rev-list",
      "--reverse",
      `--max-count=${safeDefaultCount}`,
      "HEAD",
    ]) || "";

  return fallbackText
    .split("\n")
    .map((line) => line.trim().toLowerCase())
    .filter(Boolean);
}

export function getCommitMetadata(repositoryPath, commitHash) {
  const hash = commitHash.toLowerCase();
  const subject = runGit(repositoryPath, ["show", "--quiet", "--format=%s", hash])
    .trim()
    .replace(/\s+/g, " ");
  const body = runGit(repositoryPath, ["show", "--quiet", "--format=%b", hash]).trim();
  const changedFilesText =
    runGit(repositoryPath, [
      "diff-tree",
      "--no-commit-id",
      "--name-only",
      "-r",
      hash,
    ]) || "";
  const changedFiles = changedFilesText
    .split("\n")
    .map((line) => line.trim())
    .filter(Boolean);

  return {
    hash,
    subject,
    body,
    changedFiles,
  };
}

export function getCommitPatchSnippet(repositoryPath, commitHash, maxCharacters) {
  const patchText =
    runGit(repositoryPath, ["show", "--format=", "--unified=1", commitHash]) || "";
  return truncateText(patchText, maxCharacters);
}

async function listFilesRecursively(directoryPath) {
  const directoryEntries = await fs.readdir(directoryPath, { withFileTypes: true });
  const files = [];

  for (const entry of directoryEntries) {
    const entryPath = path.join(directoryPath, entry.name);
    if (entry.isDirectory()) {
      const nestedFiles = await listFilesRecursively(entryPath);
      files.push(...nestedFiles);
      continue;
    }
    if (entry.isFile()) {
      files.push(entryPath);
    }
  }

  return files;
}

export async function getNonGeneratedDocsCorpus(repositoryRoot) {
  const docsRoot = path.join(repositoryRoot, "docs", "gdevelop5");
  const allFiles = await listFilesRecursively(docsRoot);
  const markdownFiles = allFiles
    .filter((filePath) => filePath.toLowerCase().endsWith(".md"))
    .sort();
  const corpus = [];

  for (const filePath of markdownFiles) {
    const content = await fs.readFile(filePath, "utf8");
    if (isAutoGeneratedContent(content)) {
      continue;
    }
    corpus.push({
      path: toPosixPath(path.relative(repositoryRoot, filePath)),
      content,
    });
  }

  return corpus;
}

export function tokenize(text) {
  return String(text)
    .toLowerCase()
    .split(/[^a-z0-9]+/g)
    .map((token) => token.trim())
    .filter((token) => token.length >= 4 && !STOP_WORDS.has(token));
}

export function buildKeywordSetFromCommits(commitMetadataList) {
  const keywords = new Set();

  for (const commitMetadata of commitMetadataList) {
    for (const token of tokenize(commitMetadata.subject)) {
      keywords.add(token);
    }
    for (const token of tokenize(commitMetadata.body)) {
      keywords.add(token);
    }
    for (const changedFile of commitMetadata.changedFiles) {
      for (const token of tokenize(changedFile)) {
        keywords.add(token);
      }
    }
  }

  return keywords;
}

export function selectRelevantDocumentationFiles(
  corpus,
  keywords,
  options = {},
) {
  const maxFiles = options.maxFiles || 14;
  const maxCharsPerFile = options.maxCharsPerFile || 6000;
  const totalMaxCharacters = options.totalMaxCharacters || 70000;

  const scored = corpus.map((entry) => {
    const lowerPath = entry.path.toLowerCase();
    let score = 0;
    for (const keyword of keywords) {
      if (lowerPath.includes(keyword)) {
        score += 4;
      }
    }
    return { ...entry, score };
  });

  scored.sort((left, right) => {
    if (right.score !== left.score) {
      return right.score - left.score;
    }
    return left.path.localeCompare(right.path);
  });

  const selected = [];
  let remainingCharacters = totalMaxCharacters;

  const addEntryIfPossible = (entry) => {
    if (selected.length >= maxFiles || remainingCharacters <= 0) {
      return false;
    }
    const alreadyIncluded = selected.some((candidate) => candidate.path === entry.path);
    if (alreadyIncluded) {
      return false;
    }
    const content = truncateText(entry.content, maxCharsPerFile);
    if (!content) {
      return false;
    }
    if (content.length > remainingCharacters) {
      const shortenedContent = truncateText(content, remainingCharacters);
      if (!shortenedContent) {
        return false;
      }
      selected.push({ path: entry.path, content: shortenedContent });
      remainingCharacters = 0;
      return true;
    }
    selected.push({ path: entry.path, content });
    remainingCharacters -= content.length;
    return true;
  };

  for (const entry of scored) {
    if (entry.score <= 0) {
      continue;
    }
    addEntryIfPossible(entry);
  }

  const indexFile = scored.find((entry) => entry.path === "docs/gdevelop5/index.md");
  if (indexFile) {
    addEntryIfPossible(indexFile);
  }

  for (const entry of scored) {
    if (selected.length >= maxFiles) {
      break;
    }
    addEntryIfPossible(entry);
  }

  return selected;
}

export function truncateText(text, maxCharacters) {
  if (!maxCharacters || maxCharacters < 1) {
    return "";
  }
  const source = String(text || "");
  if (source.length <= maxCharacters) {
    return source;
  }
  return `${source.slice(0, Math.max(0, maxCharacters - 25))}\n...[truncated]`;
}

export async function callAiProvider(provider, prompt) {
  const normalizedProvider = String(provider || "codex").trim().toLowerCase();
  if (normalizedProvider === "claude") {
    return callAnthropic(prompt);
  }
  if (normalizedProvider === "codex") {
    return callOpenAI(prompt);
  }
  throw new Error(
    `Unsupported AI_PROVIDER "${provider}". Expected "codex" or "claude".`,
  );
}

async function callOpenAI(prompt) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error("OPENAI_API_KEY is missing.");
  }

  const model = process.env.OPENAI_MODEL || "gpt-5-codex";
  const response = await fetch("https://api.openai.com/v1/responses", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      input: prompt,
      max_output_tokens: Number(process.env.AI_MAX_OUTPUT_TOKENS || 12000),
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`OpenAI API request failed (${response.status}): ${errorText}`);
  }

  const payload = await response.json();
  if (typeof payload.output_text === "string" && payload.output_text.trim()) {
    return payload.output_text.trim();
  }

  const parts = [];
  for (const outputItem of payload.output || []) {
    for (const contentItem of outputItem.content || []) {
      if (contentItem.type === "output_text" && typeof contentItem.text === "string") {
        parts.push(contentItem.text);
      }
    }
  }

  const text = parts.join("\n").trim();
  if (!text) {
    throw new Error("OpenAI API response did not contain any text output.");
  }
  return text;
}

async function callAnthropic(prompt) {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) {
    throw new Error("ANTHROPIC_API_KEY is missing.");
  }

  const model = process.env.ANTHROPIC_MODEL || "claude-sonnet-4-5";
  const response = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "x-api-key": apiKey,
      "anthropic-version": "2023-06-01",
      "content-type": "application/json",
    },
    body: JSON.stringify({
      model,
      max_tokens: Number(process.env.AI_MAX_OUTPUT_TOKENS || 6000),
      temperature: 0.2,
      messages: [{ role: "user", content: prompt }],
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(
      `Anthropic API request failed (${response.status}): ${errorText}`,
    );
  }

  const payload = await response.json();
  const parts = (payload.content || [])
    .filter((item) => item.type === "text" && typeof item.text === "string")
    .map((item) => item.text.trim())
    .filter(Boolean);
  const text = parts.join("\n").trim();

  if (!text) {
    throw new Error("Anthropic API response did not contain any text output.");
  }
  return text;
}

export function extractJsonPayload(text) {
  const source = String(text || "").trim();
  if (!source) {
    throw new Error("AI response is empty.");
  }

  const tryParse = (candidate) => {
    try {
      return JSON.parse(candidate);
    } catch (error) {
      return null;
    }
  };

  const direct = tryParse(source);
  if (direct) {
    return direct;
  }

  const fencedMatch = source.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fenced = tryParse(fencedMatch[1].trim());
    if (fenced) {
      return fenced;
    }
  }

  const firstBrace = source.indexOf("{");
  const lastBrace = source.lastIndexOf("}");
  if (firstBrace >= 0 && lastBrace > firstBrace) {
    const betweenBraces = tryParse(source.slice(firstBrace, lastBrace + 1));
    if (betweenBraces) {
      return betweenBraces;
    }
  }

  throw new Error("Unable to parse AI response as JSON.");
}

function isPathInsideRepository(repositoryRoot, targetPath) {
  const normalizedRoot = path.resolve(repositoryRoot);
  const normalizedTarget = path.resolve(targetPath);
  return (
    normalizedTarget === normalizedRoot ||
    normalizedTarget.startsWith(`${normalizedRoot}${path.sep}`)
  );
}

function normalizeOutputContent(content) {
  const normalized = String(content || "").replace(/\r\n/g, "\n");
  if (normalized.endsWith("\n")) {
    return normalized;
  }
  return `${normalized}\n`;
}

function getGitHeadFileContent(repositoryRoot, relativePath) {
  const output = runGit(repositoryRoot, ["show", `HEAD:${relativePath}`], {
    allowFailure: true,
  });
  if (output === null) {
    return null;
  }
  return output;
}

export async function applyDocumentationChanges(repositoryRoot, aiChanges) {
  if (!Array.isArray(aiChanges) || aiChanges.length === 0) {
    throw new Error("AI response contains no documentation changes.");
  }

  const writtenPaths = [];
  for (const change of aiChanges) {
    if (!change || typeof change.path !== "string" || typeof change.content !== "string") {
      throw new Error("Each AI change must contain string fields: path and content.");
    }

    const relativePath = toPosixPath(change.path.trim());
    if (!relativePath.startsWith("docs/gdevelop5/")) {
      throw new Error(`Disallowed path from AI output: ${relativePath}`);
    }
    if (!relativePath.toLowerCase().endsWith(".md")) {
      throw new Error(`Only markdown files can be changed: ${relativePath}`);
    }

    const absolutePath = path.resolve(repositoryRoot, relativePath);
    if (!isPathInsideRepository(repositoryRoot, absolutePath)) {
      throw new Error(`Resolved path escapes repository: ${relativePath}`);
    }

    const headFileContent = getGitHeadFileContent(repositoryRoot, relativePath);
    if (headFileContent !== null && isAutoGeneratedContent(headFileContent)) {
      throw new Error(
        `Auto-generated files cannot be edited (from HEAD): ${relativePath}`,
      );
    }

    let existingContent = null;
    try {
      existingContent = await fs.readFile(absolutePath, "utf8");
    } catch (error) {
      if (!error || error.code !== "ENOENT") {
        throw error;
      }
    }
    if (existingContent !== null && isAutoGeneratedContent(existingContent)) {
      throw new Error(
        `Auto-generated files cannot be edited (from working tree): ${relativePath}`,
      );
    }

    const outputContent = normalizeOutputContent(change.content);
    await fs.mkdir(path.dirname(absolutePath), { recursive: true });
    await fs.writeFile(absolutePath, outputContent, "utf8");
    writtenPaths.push(relativePath);
  }

  return [...new Set(writtenPaths)];
}

export async function enforceAllowedChangedFiles(repositoryRoot) {
  const changedFilesText = runGit(repositoryRoot, ["diff", "--name-only"]) || "";
  const changedFiles = changedFilesText
    .split("\n")
    .map((line) => line.trim())
    .filter(Boolean);

  for (const relativePath of changedFiles) {
    if (relativePath === "automated_updates_data.json") {
      continue;
    }

    if (!relativePath.startsWith("docs/gdevelop5/")) {
      throw new Error(`Unexpected file changed by automation: ${relativePath}`);
    }

    const absolutePath = path.join(repositoryRoot, relativePath);
    const currentContent = await fs.readFile(absolutePath, "utf8");
    const headContent = getGitHeadFileContent(repositoryRoot, relativePath);

    if (isAutoGeneratedContent(currentContent)) {
      throw new Error(`Auto-generated file was edited: ${relativePath}`);
    }
    if (headContent !== null && isAutoGeneratedContent(headContent)) {
      throw new Error(`Auto-generated file from HEAD was edited: ${relativePath}`);
    }
  }

  return changedFiles;
}

export function formatCommitSummaryForPrompt(commitMetadataList) {
  const lines = [];

  for (const commitMetadata of commitMetadataList) {
    lines.push(`- ${commitMetadata.hash} :: ${commitMetadata.subject}`);
    if (commitMetadata.body) {
      const oneLineBody = commitMetadata.body.replace(/\s+/g, " ").trim();
      if (oneLineBody) {
        lines.push(`  Body: ${truncateText(oneLineBody, 280)}`);
      }
    }
    if (commitMetadata.changedFiles.length > 0) {
      lines.push(
        `  Files (${commitMetadata.changedFiles.length}): ${commitMetadata.changedFiles.join(", ")}`,
      );
    }
  }

  return lines.join("\n");
}

export function formatDocsForPrompt(selectedDocs) {
  return selectedDocs
    .map((entry) => {
      return `--- FILE: ${entry.path}\n${entry.content}`;
    })
    .join("\n\n");
}

export function pickScopeCandidatesFromCorpus(corpus) {
  const scopeSet = new Set();

  for (const entry of corpus) {
    const withoutPrefix = entry.path.replace(/^docs\/gdevelop5\//, "");
    const segments = withoutPrefix.split("/").filter(Boolean);
    if (segments.length === 0) {
      continue;
    }

    if (segments[0].toLowerCase().endsWith(".md")) {
      scopeSet.add(segments[0]);
      continue;
    }

    if (segments.length >= 2) {
      scopeSet.add(`${segments[0]}/${segments[1]}`);
    } else {
      scopeSet.add(segments[0]);
    }
  }

  return [...scopeSet].sort((left, right) => left.localeCompare(right));
}

export function pickKeywordsFromScope(scope) {
  return new Set(tokenize(scope));
}

export function listRepositoryFiles(repositoryPath) {
  const text = runGit(repositoryPath, ["ls-files"]) || "";
  return text
    .split("\n")
    .map((line) => line.trim())
    .filter(Boolean);
}

function isLikelyTextFile(filePath) {
  const extension = path.extname(filePath).toLowerCase();
  if (!extension) {
    return true;
  }
  return !BINARY_FILE_EXTENSIONS.has(extension);
}

export async function buildCodeSnippetsForKeywords(
  repositoryPath,
  keywordSet,
  options = {},
) {
  const maxFiles = options.maxFiles || 12;
  const maxCharsPerFile = options.maxCharsPerFile || 4000;
  const allFiles = listRepositoryFiles(repositoryPath);
  const keywords = [...keywordSet];
  const scored = [];

  for (const relativePath of allFiles) {
    if (!isLikelyTextFile(relativePath)) {
      continue;
    }
    const lowerPath = relativePath.toLowerCase();
    let score = 0;
    for (const keyword of keywords) {
      if (lowerPath.includes(keyword)) {
        score += 3;
      }
    }
    if (score > 0) {
      scored.push({ relativePath, score });
    }
  }

  scored.sort((left, right) => {
    if (right.score !== left.score) {
      return right.score - left.score;
    }
    return left.relativePath.localeCompare(right.relativePath);
  });

  const selected = scored.slice(0, maxFiles);
  const snippets = [];
  for (const entry of selected) {
    const absolutePath = path.join(repositoryPath, entry.relativePath);
    try {
      const content = await fs.readFile(absolutePath, "utf8");
      snippets.push({
        path: entry.relativePath,
        content: truncateText(content, maxCharsPerFile),
      });
    } catch (error) {
      if (error && error.code === "EISDIR") {
        continue;
      }
      throw error;
    }
  }

  return snippets;
}

export function formatCodeSnippetsForPrompt(snippets) {
  return snippets
    .map((entry) => {
      return `--- CODE FILE: ${entry.path}\n${entry.content}`;
    })
    .join("\n\n");
}
